{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика 1\n",
    "\n",
    "## Введение\n",
    "\n",
    "На этой неделе было много новой информации. Чтобы всё это запомнилось, необходимо попрактиковаться.\n",
    "\n",
    "В этом нам поможет главный персонаж нашего курса, существо по имени Зюк (Zookee) с далёкой планеты Нейра.\n",
    "Нейряне не так уж сильно отличаются от землян, но для того, чтобы чему-то научиться, им каждый раз приходится формировать набор данных и сознательно программировать небольшую нейросеть в своём мозгу. В этом есть свои плюсы и минусы, но обсуждение этих вопросов выходит далеко за рамки нашего курса.\n",
    "\n",
    "Важно что Зюк очень любит путешествовать и недавно он прилетел к нам.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stacymiller/stepic_neural_networks_public/master/files/ship_bq.jpg\" width=500/>\n",
    "\n",
    "Мы, конечно, обрадовались его визиту, ведь сразу было видно, что он очень добрый, общительный и, конечно, необычный. Мы договорились, что будем помогать ему собирать данные, а он поделится своим опытом программирования нейросетей. Мы немало удивились, узнав, что многие из его нейросетей написаны на Python. По его выражению, Python - это лучшее из того, что придумали земляне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перцептрон\n",
    "\n",
    "После небольшой прогулки Зюк сильно проголодался. Кроме того, оказалось, что он очень привередлив в еде:\n",
    "едва попробовав очередное блюдо, он менялся в цвете и вежливо отказывался. К счастью, ему пришлись по нраву яблоки, и вопрос с едой был решен.\n",
    "\n",
    "Однако в наше отсутствие Зюк оставался совершенно беспомощным, поскольку, гуляя в саду, он не мог отличить яблоки от груш (которые были ему совершенно ненавистны), не попробовав их на вкус.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stacymiller/stepic_neural_networks_public/master/files/apples.jpg\" width=300 />\n",
    "\n",
    "Сформировав путём горьких ошибок небольшой набор данных, Зюк достал какое-то странное устройство и начал программировать перцептрон. Мы вызвались помочь, и он, улыбнувшись, согласился.\n",
    "\n",
    "По его словам, задача должна была оказаться достаточно простой, учитывая что у него в запасе были сети, способные извлекать такие высокоуровневые характеристики, как симметричность и желтизна объекта.\n",
    "\n",
    "### Загрузка данных\n",
    "\n",
    "Первым делом мы решили подготовить всё, что нам может понадобится, и разобраться с набором данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as p3\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from ipywidgets import interact, RadioButtons, IntSlider, FloatSlider, Dropdown, BoundedFloatText\n",
    "from numpy.linalg import norm\n",
    "\n",
    "random.seed(42) # начальное состояние генератора случайных чисел, чтобы можно было воспроизводить результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "data = np.loadtxt(\"data.csv\", delimiter=\",\")\n",
    "pears = data[:, 2] == 1\n",
    "apples = np.logical_not(pears)\n",
    "plt.scatter(data[apples][:, 0], data[apples][:, 1], color = \"red\")\n",
    "plt.scatter(data[pears][:, 0], data[pears][:, 1], color = \"green\")\n",
    "plt.xlabel(\"yellowness\")\n",
    "plt.ylabel(\"symmetry\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные -  матрица $1000 \\times 3$, в каждой строке указаны желтизна и симметричность очередного фрукта (в первой и второй колонках) и его сортовая принадлежность (третья колонка, 0 - яблоки, 1 - груши)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача действительно выглядела не очень сложно.\n",
    "\n",
    "Когда данные были загружены, мы захотели впечатлить Зюка своими познаниями и сразу ринулись писать алгоритм обучения перцептрона, однако Зюк остановил нас, напомнив, что сперва нужно решить, какой вообще будет структура модуля, распознающего яблоки и груши. Мы призадумались, и он с ностальгической улыбкой показал нам одну из своих заготовок:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перцептрон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, w, b):\n",
    "        \"\"\"\n",
    "        Инициализируем наш объект - перцептрон.\n",
    "        w - вектор весов размера (m, 1), где m - количество переменных\n",
    "        b - число\n",
    "        \"\"\"\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def forward_pass(self, single_input):\n",
    "        \"\"\"\n",
    "        Метод рассчитывает ответ перцептрона при предъявлении одного примера\n",
    "        single_input - вектор примера размера (m, 1).\n",
    "        Метод возвращает число (0 или 1) или boolean (True/False)\n",
    "        \"\"\"\n",
    "        \n",
    "        result = 0\n",
    "        for i in range(0, len(self.w)):\n",
    "            result += self.w[i] * single_input[i]\n",
    "        result += self.b\n",
    "        \n",
    "        if result > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Метод рассчитывает ответ перцептрона при предъявлении набора примеров\n",
    "        input_matrix - матрица примеров размера (n, m), каждая строка - отдельный пример,\n",
    "        n - количество примеров, m - количество переменных\n",
    "        Возвращает вертикальный вектор размера (n, 1) с ответами перцептрона\n",
    "        (элементы вектора - boolean или целые числа (0 или 1))\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Этот метод необходимо реализовать\n",
    "        return np.dot(input_matrix, self.w) + self.b > 0\n",
    "    \n",
    "    def train_on_single_example(self, example, y):\n",
    "        \"\"\"\n",
    "        принимает вектор активации входов example формы (m, 1) \n",
    "        и правильный ответ для него (число 0 или 1 или boolean),\n",
    "        обновляет значения весов перцептрона в соответствии с этим примером\n",
    "        и возвращает размер ошибки, которая случилась на этом примере до изменения весов (0 или 1)\n",
    "        (на её основании мы потом построим интересный график)\n",
    "        \"\"\"\n",
    "\n",
    "        ## Этот метод необходимо реализовать\n",
    "        predict = self.vectorized_forward_pass(example.transpose())\n",
    "        delta = y - predict\n",
    "        self.w += delta * example\n",
    "        self.b += delta\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оказалось, что это был первый нейрон, который он запрограммировал сам. Он рассказал, что метод `train_on_single_example` остался без реализации, так как в то время веса ему еще готовила мама. Векторизовать он, как и все дети, не любил, поэтому данный метод тоже не готов.\n",
    "\n",
    "Мы с лёгким нетерпением спросили, можно ли нам что-нибудь запрограммировать. Убедившись, что мы разобрались в приведённом коде, Зюк поставил задачу:\n",
    "\n",
    "### Реализовать метод `vectorized_forward_pass`\n",
    "\n",
    "Это метод, который считает значения активационной функции нейрона. \n",
    "\n",
    "На вход он принимает матрицу активаций размера $n \\times m$ и вектор ответов длины $n$. Каждая строка - отдельный пример. В случае с яблоками и грушами матрица будет размера $n \\times 2$, где $n$ - количество примеров, для которых мы хотим получить решения (но, конечно, когда мы будем проверять ваши решения, у данных может быть и другой размер!).  `vectorized_forward_pass` отдаёт столбец (формы $n\\times 1$) значений активационной функции нейрона для всех переданных ему входных данных. Зюк предупредил, что если мы будем использовать **циклы** (`for`, `while`) или **операторы ветвления** (`if`) - **решение не пройдёт его внутреннюю проверку**.\n",
    "\n",
    "Было что-то занудное в его голосе, но что делать? Видимо, это та цена, которую нужно платить, если хочешь, чтобы тебя поняли.\n",
    "\n",
    "/// Чтобы всё работало - впишите свою реализацию в предоставленный выше шаблон и сдайте функцию на сайте курса, чтобы <br>\n",
    "/// проверить, что решение правильное. Имейте в виду: наши тесты не охватывают всех возможных характеристик, <br>\n",
    "/// поэтому постарайтесь делать качественно, иначе незамеченные ошибки могут накапливаться. <br>\n",
    "/// Не забудьте запустить код в ячейке, иначе Python будет помнить старую версию того, что было написано"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование\n",
    "\n",
    "Мы с непривычки здорово намучились, и у нас назрел вопрос: а так ли нужна эта векторизация для классификации яблок?\n",
    "\n",
    "\\- Смотря насколько вы цените своё время, - задумчиво ответил Зюк, - признаюсь, я и сам уже привык\n",
    "воспринимать векторизацию как что-то само собой разумеющееся, впитанное с молоком матери, как у вас говорят. \n",
    "\n",
    "Вдруг что-то озорное появилось в его глазах, и он весело предложил: \"Так давайте проверим!\".\n",
    "\n",
    "Выхватив пульт управления, Зюк начал что-то быстро печатать. Не прошло и минуты, как он гордо представил нам свои результаты:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_perceptron(m):\n",
    "    \"\"\"Создаём перцептрон со случайными весами и m входами\"\"\"\n",
    "    w = np.random.random((m, 1))\n",
    "    return Perceptron(w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_v_f_p(n, m):\n",
    "    \"\"\"\n",
    "    Расчитывает для перцептрона с m входами\n",
    "    с помощью методов forward_pass и vectorized_forward_pass\n",
    "    n ответов перцептрона на случайных данных.\n",
    "    Возвращает время, затраченное vectorized_forward_pass и forward_pass\n",
    "    на эти расчёты.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = create_perceptron(m)\n",
    "    input_m = np.random.random_sample((n, m))\n",
    "    \n",
    "    start = time.time()\n",
    "    vec = p.vectorized_forward_pass(input_m)\n",
    "    end = time.time()\n",
    "    vector_time = end - start\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(0, n):\n",
    "        p.forward_pass(input_m[i]) \n",
    "    end = time.time()\n",
    "    plain_time = end - start\n",
    "\n",
    "    return [vector_time, plain_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_execution_time(n, m, trials=100):\n",
    "    \"\"\"среднее время выполнения forward_pass и vectorized_forward_pass за trials испытаний\"\"\"\n",
    "    \n",
    "    return np.array([test_v_f_p(m, n) for _ in range(trials)]).mean(axis=0)\n",
    "\n",
    "def plot_mean_execution_time(n, m):\n",
    "    \"\"\"рисует графики среднего времени выполнения forward_pass и vectorized_forward_pass\"\"\"\n",
    "    \n",
    "    mean_vectorized, mean_plain = mean_execution_time(int(n), int(m))\n",
    "    p1 = plt.bar([0], mean_vectorized,  color='g')\n",
    "    p2 = plt.bar([1], mean_plain, color='r')\n",
    "\n",
    "    plt.ylabel(\"Time spent\")\n",
    "    plt.yticks(np.arange(0, mean_plain))\n",
    "\n",
    "    plt.xticks(range(0,1))\n",
    "    plt.legend((\"vectorized\",\"non - vectorized\"))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_mean_execution_time, \n",
    "            n=RadioButtons(options=[\"1\", \"10\", \"100\"]),\n",
    "            m=RadioButtons(options=[\"1\", \"10\", \"100\"], separator=\" \"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы принялись увлечённо экспериментировать, чтобы лучше понять, какова разница во времени работы у векторизованного и не векторизованного вариантов при разных `n` и `m`. Нетрудно заметить, что чем больше у нас данных, тем более впечатляющей эта разница становилась. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение перцептрона\n",
    "\n",
    "Отлично, осталось научить наш перцептрон хорошо различать груши и яблоки - и Зюк сможет гулять в саду, не оставаясь при этом голодным! Давайте напишем код, который, приняв на вход пример и правильный ответ, будет изменять веса в соответствии с правилом обучения перцептрона. Это метод `train_on_single_example`, который вам и нужно будет сейчас реализовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, w, b):\n",
    "        \"\"\"\n",
    "        Инициализируем наш объект - перцептрон.\n",
    "        w - вектор весов размера (m, 1), где m - количество переменных\n",
    "        b - число\n",
    "        \"\"\"\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def forward_pass(self, single_input):\n",
    "        \"\"\"\n",
    "        Метод рассчитывает ответ перцептрона при предъявлении одного примера\n",
    "        single_input - вектор примера размера (m, 1).\n",
    "        Метод возвращает число (0 или 1) или boolean (True/False)\n",
    "        \"\"\"\n",
    "        \n",
    "        result = 0\n",
    "        for i in range(0, len(self.w)):\n",
    "            result += self.w[i] * single_input[i]\n",
    "        result += self.b\n",
    "        \n",
    "        if result > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "            \"\"\"\n",
    "            Метод рассчитывает ответ перцептрона при предъявлении набора примеров\n",
    "            input_matrix - матрица примеров размера (n, m), каждая строка - отдельный пример,\n",
    "            n - количество примеров, m - количество переменных\n",
    "            Возвращает вертикальный вектор размера (n, 1) с ответами перцептрона\n",
    "            (элементы вектора - boolean или целые числа (0 или 1))\n",
    "            \"\"\"\n",
    "            \n",
    "            ## Этот метод необходимо реализовать\n",
    "            return np.dot(input_matrix, self.w) + self.b > 0\n",
    "        \n",
    "    def train_on_single_example(self, example, y):\n",
    "        \"\"\"\n",
    "        принимает вектор активации входов example формы (m, 1) \n",
    "        и правильный ответ для него (число 0 или 1 или boolean),\n",
    "        обновляет значения весов перцептрона в соответствии с этим примером\n",
    "        и возвращает размер ошибки, которая случилась на этом примере до изменения весов (0 или 1)\n",
    "        (на её основании мы потом построим интересный график)\n",
    "        \"\"\"\n",
    "\n",
    "        ## Этот метод необходимо реализовать\n",
    "        predict = self.vectorized_forward_pass(example.transpose())\n",
    "        delta = y - predict\n",
    "        self.w += delta * example\n",
    "        self.b += delta\n",
    "        return delta\n",
    "\n",
    "    def train_until_convergence(self, input_matrix, y, max_steps=1e8):\n",
    "        \"\"\"\n",
    "        input_matrix - матрица входов размера (n, m),\n",
    "        y - вектор правильных ответов размера (n, 1) (y[i] - правильный ответ на пример input_matrix[i]),\n",
    "        max_steps - максимальное количество шагов.\n",
    "        Применяем train_on_single_example, пока не перестанем ошибаться или до умопомрачения.\n",
    "        Константа max_steps - наше понимание того, что считать умопомрачением.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        errors = 1\n",
    "        while errors and i < max_steps:\n",
    "            i += 1\n",
    "            errors = 0\n",
    "            for example, answer in zip(input_matrix, y):\n",
    "                example = example.reshape((example.size, 1))\n",
    "                error = self.train_on_single_example(example, answer)\n",
    "                errors += int(error)  # int(True) = 1, int(False) = 0, так что можно не делать if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация обучения\n",
    "\n",
    "Отлично! С вашей помощью у Зюка появился перцептрон, который умеет учиться. В благодарность Зюк подготовил несколько функций, которые помогут посмотреть в динамике на то, как протекает процесс обучения перцептрона. \n",
    "\n",
    "/// В данном коде детально разбираться не обязательно, главное запустить каждую ячейку и посмотреть на анимацию.\n",
    "<br>/// Не забудьте остановить анимацию, иначе подвисание через полчаса-час гарантировано."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_line(coefs):\n",
    "    \"\"\"\n",
    "    рисует разделяющую прямую, соответствующую весам, переданным в coefs = (weights, bias), \n",
    "    где weights - ndarray формы (2, 1), bias - число\n",
    "    \"\"\"\n",
    "    w, bias = coefs\n",
    "    a, b = - w[0][0] / w[1][0], - bias / w[1][0]\n",
    "    xx = np.linspace(*plt.xlim())\n",
    "    line.set_data(xx, a*xx + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_by_step_weights(p, input_matrix, y, max_steps=1e6):\n",
    "    \"\"\"\n",
    "    обучает перцептрон последовательно на каждой строчке входных данных, \n",
    "    возвращает обновлённые веса при каждом их изменении\n",
    "    p - объект класса Perceptron\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    errors = 1\n",
    "    while errors and i < max_steps:\n",
    "        i += 1\n",
    "        errors = 0\n",
    "        for example, answer in zip(input_matrix, y):\n",
    "            example = example.reshape((example.size, 1))\n",
    "            \n",
    "            error = p.train_on_single_example(example, answer)\n",
    "            errors += error  # здесь мы упадём, если вы забыли вернуть размер ошибки из train_on_single_example\n",
    "            if error:  # будем обновлять положение линии только тогда, когда она изменила своё положение\n",
    "                yield p.w, p.b\n",
    "                \n",
    "    for _ in range(20): yield p.w, p.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "np.random.seed(1)\n",
    "fig = plt.figure()\n",
    "plt.scatter(data[apples][:, 0], data[apples][:, 1], color = \"red\", marker=\".\", label=\"Apples\")\n",
    "plt.scatter(data[pears][:, 0], data[pears][:, 1], color = \"green\", marker=\".\", label=\"Pears\")\n",
    "plt.xlabel(\"yellowness\")\n",
    "plt.ylabel(\"symmetry\")\n",
    "line, = plt.plot([], [], color=\"black\", linewidth=2)  # создаём линию, которая будет показывать границу разделения\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "perceptron_for_weights_line = create_perceptron(2)  # создаём перцептрон нужной размерности со случайными весами\n",
    "\n",
    "from functools import partial\n",
    "weights_ani = partial(\n",
    "    step_by_step_weights, p=perceptron_for_weights_line, input_matrix=data[:, :-1], y=data[:, -1][:,np.newaxis]\n",
    ")  # про partial почитайте на https://docs.python.org/3/library/functools.html#functools.partial\n",
    "\n",
    "ani = FuncAnimation(fig, func=plot_line, frames=weights_ani, blit=False, interval=10, repeat=True)\n",
    "# если Jupyter не показывает вам анимацию - раскомментируйте строчку ниже и посмотрите видео\n",
    "# ani.save(\"perceptron_seeking_for_solution.mp4\", fps=15)\n",
    "plt.show()\n",
    "\n",
    "## Не забудьте остановить генерацию новых картинок, прежде чем идти дальше (кнопка \"выключить\" в правом верхнем углу графика)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Он также подготовил несколько примеров, чтобы проиллюстрировать философию обучения перцептрона, которая не всегда интуитивна.\n",
    "\n",
    "Во-первых, количество неправильно классифицированных примеров не всегда уменьшается. То есть в процессе обучения у перцептрона могут случаться \"взлёты и падения\" - делился мудростью Зюк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step_by_step_errors(p, input_matrix, y, max_steps=1e6):\n",
    "    \"\"\"\n",
    "    обучает перцептрон последовательно на каждой строчке входных данных, \n",
    "    на каждом шаге обучения запоминает количество неправильно классифицированных примеров\n",
    "    и возвращает список из этих количеств\n",
    "    \"\"\"\n",
    "    def count_errors():\n",
    "        return np.abs(p.vectorized_forward_pass(input_matrix).astype(int) - y).sum()\n",
    "    errors_list = [count_errors()]\n",
    "    i = 0\n",
    "    errors = 1\n",
    "    while errors and i < max_steps:\n",
    "        i += 1\n",
    "        errors = 0\n",
    "        for example, answer in zip(input_matrix, y):\n",
    "            example = example.reshape((example.size, 1))\n",
    "            \n",
    "            error = p.train_on_single_example(example, answer)\n",
    "            errors += error\n",
    "            errors_list.append(count_errors())\n",
    "    return errors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "perceptron_for_misclassification = create_perceptron(2)\n",
    "errors_list = step_by_step_errors(perceptron_for_misclassification, input_matrix=data[:, :-1], y=data[:, -1][:,np.newaxis])\n",
    "plt.plot(errors_list);\n",
    "plt.ylabel(\"Number of errors\")\n",
    "plt.xlabel(\"Algorithm step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы испугались, что что-то идёт не так, но Зюк успокоил нас, сказав что такой страшный график - следствие нашей философии. Ведь философия обучения перцептрона в том, что лучше будут становиться веса, а не ответы.\n",
    "\n",
    "\"А как именно веса становятся лучше?\" - вот вопрос, который теперь волновал каждого из нас. Мы решили посмотреть, как перцептрон научился тому, чему он научился. Возьмём тот вектор весов, к которым перцептрон сошёлся, и посмотрим, как изменяется расстояние до него, пока перцептрон учится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector(p):\n",
    "    \"\"\"возвращает вектор из всех весов перцептрона, включая смещение\"\"\"\n",
    "    v = np.array(list(p.w.ravel()) + [p.b])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_by_step_distances(p, ideal, input_matrix, y, max_steps=1e6):\n",
    "    \"\"\"обучает перцептрон p и записывает каждое изменение расстояния от текущих весов до ideal\"\"\"\n",
    "    distances = [norm(get_vector(p) - ideal)]\n",
    "    i = 0\n",
    "    errors = 1\n",
    "    while errors and i < max_steps:\n",
    "        i += 1\n",
    "        errors = 0\n",
    "        for example, answer in zip(input_matrix, y):\n",
    "            example = example.reshape((example.size, 1))\n",
    "            \n",
    "            error = p.train_on_single_example(example, answer)\n",
    "            errors += error\n",
    "            if error:\n",
    "                distances.append(norm(get_vector(p) - ideal))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "init_weights = np.random.random_sample(3)\n",
    "w, b = init_weights[:-1].reshape((2, 1)), init_weights[-1]\n",
    "ideal_p = Perceptron(w.copy(), b.copy())\n",
    "ideal_p.train_until_convergence(data[:, :-1], data[:, -1][:,np.newaxis])\n",
    "ideal_weights = get_vector(ideal_p)\n",
    "\n",
    "new_p = Perceptron(w.copy(), b.copy())\n",
    "distances = step_by_step_distances(new_p, ideal_weights, data[:, :-1], data[:, -1][:,np.newaxis])\n",
    "\n",
    "plt.xlabel(\"Number of weight updates\")\n",
    "plt.ylabel(\"Distance between good and current weights\")\n",
    "plt.plot(distances);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и можно было догадаться ещё по тому, как вела себя прямая на нашей анимированной картинке, вектор весов постепенно приближается к множеству хороших весов, хотя тоже \"не всё гладко\". Постарайтесь понять, почему, вспомнив доказательство сходимости перцептрона и понятие \"векторов весов, хороших \"с запасом\"\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покончив с перцептроном, Зюк несколько торопливо предложил посмотреть, как в подобной ситуации себя проявит логистический нейрон, основанный на градиентном спуске. В ответ на наши недоумевающие взгляды Зюк смущённо объяснил, что очень боится, что ему когда-нибудь попадётся какое-то \"необычное\" яблоко или груша. Тогда данные могут перестать быть линейно разделимыми, и он рискует зависнуть, обучая несходящийся перцептрон. Для взрослого Нейрянина допустить такое - большой позор.\n",
    "\n",
    "Конечно, можно поставить ограничение на число шагов алгоритма... Но если вспомнить, как выглядит график количества ошибок, сразу станет понятно, что мы рискуем получить очень плохой классификатор, если оборвём процесс обучения раньше, чем нужно.\n",
    "\n",
    "Мы пропросили Зюка не подсказывать нам слишком много на этот раз. Всё-таки мы хотим сами научиться программировать нейросети.\n",
    "Зюк вежливо согласился, но настоял, чтобы мы посмотрели на его заготовку. Ключевые моменты он спрятал, предоставив нам возможность попрактиковаться.\n",
    "\n",
    "/// Изучите код и реализуйте пропущенные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Определим разные полезные функции\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"сигмоидальная функция, работает и с числами, и с векторами (поэлементно)\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"производная сигмоидальной функции, работает и с числами, и с векторами (поэлементно)\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Neuron нас немного напугал... Такая махина. Зюк заверил нас, что он только кажется большим. На самом деле можно было всё это написать в три-четыре строчки, но вот понять их - это было бы сложнее. Он также извинился за многословность и неоптимальность реализации, объясняя её тем, что \"на таком удобнее учиться\". Пришлось поверить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, weights, activation_function=sigmoid, activation_function_derivative=sigmoid_prime):\n",
    "        \"\"\"\n",
    "        weights - вертикальный вектор весов нейрона формы (m, 1), weights[0][0] - смещение\n",
    "        activation_function - активационная функция нейрона, сигмоидальная функция по умолчанию\n",
    "        activation_function_derivative - производная активационной функции нейрона\n",
    "        \"\"\"\n",
    "        \n",
    "        assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "        \n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        \n",
    "    def forward_pass(self, single_input):\n",
    "        \"\"\"\n",
    "        активационная функция логистического нейрона\n",
    "        single_input - вектор входов формы (m, 1), \n",
    "        первый элемент вектора single_input - единица (если вы хотите учитывать смещение)\n",
    "        \"\"\"\n",
    "        \n",
    "        result = 0\n",
    "        for i in range(self.w.size):\n",
    "            result += float(self.w[i] * single_input[i])\n",
    "        return self.activation_function(result)\n",
    "    \n",
    "    def summatory(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Вычисляет результат сумматорной функции для каждого примера из input_matrix. \n",
    "        input_matrix - матрица примеров размера (n, m), каждая строка - отдельный пример,\n",
    "        n - количество примеров, m - количество переменных.\n",
    "        Возвращает вектор значений сумматорной функции размера (n, 1).\n",
    "        \"\"\"\n",
    "        # Этот метод необходимо реализовать\n",
    "        \n",
    "        return np.dot(input_matrix, self.w)\n",
    "    \n",
    "    def activation(self, summatory_activation):\n",
    "        \"\"\"\n",
    "        Вычисляет для каждого примера результат активационной функции,\n",
    "        получив на вход вектор значений сумматорной функций\n",
    "        summatory_activation - вектор размера (n, 1), \n",
    "        где summatory_activation[i] - значение суммматорной функции для i-го примера.\n",
    "        Возвращает вектор размера (n, 1), содержащий в i-й строке \n",
    "        значение активационной функции для i-го примера.\n",
    "        \"\"\"\n",
    "        # Этот метод необходимо реализовать\n",
    "        vfunc = np.vectorize(self.activation_function)\n",
    "        return vfunc(summatory_activation)\n",
    "    \n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Векторизованная активационная функция логистического нейрона.\n",
    "        input_matrix - матрица примеров размера (n, m), каждая строка - отдельный пример,\n",
    "        n - количество примеров, m - количество переменных.\n",
    "        Возвращает вертикальный вектор размера (n, 1) с выходными активациями нейрона\n",
    "        (элементы вектора - float)\n",
    "        \"\"\"\n",
    "        return self.activation(self.summatory(input_matrix))\n",
    "        \n",
    "    def SGD(self, X, y, batch_size, learning_rate=0.1, eps=1e-6, max_steps=200):\n",
    "        \"\"\"\n",
    "        Внешний цикл алгоритма градиентного спуска.\n",
    "        X - матрица входных активаций (n, m)\n",
    "        y - вектор правильных ответов (n, 1)\n",
    "        \n",
    "        learning_rate - константа скорости обучения\n",
    "        batch_size - размер батча, на основании которого \n",
    "        рассчитывается градиент и совершается один шаг алгоритма\n",
    "        \n",
    "        eps - критерий остановки номер один: если разница между значением целевой функции \n",
    "        до и после обновления весов меньше eps - алгоритм останавливается. \n",
    "        Вторым вариантом была бы проверка размера градиента, а не изменение функции,\n",
    "        что будет работать лучше - неочевидно. В заданиях используйте первый подход.\n",
    "        \n",
    "        max_steps - критерий остановки номер два: если количество обновлений весов \n",
    "        достигло max_steps, то алгоритм останавливается\n",
    "        \n",
    "        Метод возвращает 1, если отработал первый критерий остановки (спуск сошёлся) \n",
    "        и 0, если второй (спуск не достиг минимума за отведённое время).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Этот метод необходимо реализовать\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def update_mini_batch(self, X, y, learning_rate, eps):\n",
    "        \"\"\"\n",
    "        X - матрица размера (batch_size, m)\n",
    "        y - вектор правильных ответов размера (batch_size, 1)\n",
    "        learning_rate - константа скорости обучения\n",
    "        eps - критерий остановки номер один: если разница между значением целевой функции \n",
    "        до и после обновления весов меньше eps - алгоритм останавливается. \n",
    "        \n",
    "        Рассчитывает градиент (не забывайте использовать подготовленные заранее внешние функции) \n",
    "        и обновляет веса нейрона. Если ошибка изменилась меньше, чем на eps - возвращаем 1, \n",
    "        иначе возвращаем 0.\n",
    "        \"\"\"\n",
    "        # Этот метод необходимо реализовать\n",
    "        z = self.summatory(X)\n",
    "        y_hat = self.activation(z)\n",
    "        loss = 0.5 * np.mean((y_hat - y) ** 2)\n",
    "        # grad = compute_grad_analytically(self, X, y)\n",
    "        grad = (((y_hat - y)/len(y)) * self.activation_function_derivative(z)).transpose().dot(X)\n",
    "        grad = grad.transpose()\n",
    "        self.w -= learning_rate * grad\n",
    "        new_y_hat =self.activation(self.summatory(X))\n",
    "        new_loss = 0.5 * np.mean((new_y_hat - y) ** 2)\n",
    "        if np.abs(new_loss - loss) > eps:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зюк добавил, что на практике стохастический градиентный спуск обычно ограничивается не по количеству шагов алгоритма, а по количеству эпох обучения. Мы с ужасом переглянулись. Выходит, что Зюк просто не понимает, что земляне редко живут дольше ста лет... \"Зюк, - грустно сказали мы, - скорее всего, нам не удастся дождаться окончания работы алгоритма, люди столько не живут\". Зюк сперва удивился, потом рассмеялся и объяснил: эпохой обучения называют предъявление всех примеров по одному разу. Обычно батчи формируют так: входные данные перемешиваются, после этого разбиваются на кусочки по batch_size штук в каждом. После того как все примеры хотя бы раз побывали в батче, данные перемешиваются снова. Мы облегчённо вздохнули. Сейчас нам интересно понаблюдать за отдельными изменениями весов, поэтому мы ограничиваем алгоритм не эпохами, а количеством обновлений весов.\n",
    "\n",
    "Далее он показал нам код для расчета целевой функции и её градиента. Приятно было осознавать, что целевую функцию можно будет очень легко заменить, если нам захочется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J_quadratic(neuron, X, y):\n",
    "    \"\"\"\n",
    "    Оценивает значение квадратичной целевой функции.\n",
    "    Всё как в лекции, никаких хитростей.\n",
    "\n",
    "    neuron - нейрон, у которого есть метод vectorized_forward_pass, предсказывающий значения на выборке X\n",
    "    X - матрица входных активаций (n, m)\n",
    "    y - вектор правильных ответов (n, 1)\n",
    "        \n",
    "    Возвращает значение J (число)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y.shape[1] == 1, 'Incorrect y shape'\n",
    "    \n",
    "    return 0.5 * np.mean((neuron.vectorized_forward_pass(X) - y) ** 2)\n",
    "\n",
    "def J_quadratic_derivative(y, y_hat):\n",
    "    \"\"\"\n",
    "    Вычисляет вектор частных производных целевой функции по каждому из предсказаний.\n",
    "    y_hat - вертикальный вектор предсказаний,\n",
    "    y - вертикальный вектор правильных ответов,\n",
    "    \n",
    "    В данном случае функция смехотворно простая, но если мы захотим поэкспериментировать \n",
    "    с целевыми функциями - полезно вынести эти вычисления в отдельный этап.\n",
    "    \n",
    "    Возвращает вектор значений производной целевой функции для каждого примера отдельно.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y_hat.shape == y.shape and y_hat.shape[1] == 1, 'Incorrect shapes'\n",
    "    \n",
    "    return (y_hat - y) / len(y)\n",
    "    \n",
    "def compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "    \"\"\"\n",
    "    Аналитическая производная целевой функции\n",
    "    neuron - объект класса Neuron\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений\n",
    "    y - правильные ответы для примеров из матрицы X\n",
    "    J_prime - функция, считающая производные целевой функции по ответам\n",
    "    \n",
    "    Возвращает вектор размера (m, 1)\n",
    "    \"\"\"\n",
    "    grad = (((predict - y)/len(y)) * self.activation_function_derivative(predict)).transpose().dot(X)\n",
    "    grad = grad.transpose()\n",
    "     \n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "    # осознайте эту строчку:\n",
    "    dz_dw = X\n",
    "\n",
    "    # а главное, эту:\n",
    "    grad = ((dy_dyhat * dyhat_dz).T).dot(dz_dw)\n",
    "    \n",
    "    # можно было написать в два этапа. Осознайте, почему получается одно и то же\n",
    "    # grad_matrix = dy_dyhat * dyhat_dz * dz_dw\n",
    "    # grad = np.sum(, axis=0)\n",
    "    \n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    grad = grad.T\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако нам в голову пришла тревожная мысль.. А вдруг при расчёте градиента допущена ошибка?  \n",
    "\n",
    "Как мы говорили в лекциях, неплохо бы проверить, правильно ли мы считаем градиенты, до того, как куда-то по этим градиентам спускаться. Зюк, например, долго пытался понять, как нам удалось сделать тот градиентный спуск, что мы показывали вам в лекции под саундтрек из \"Секретных материалов\". А мы всего-то забыли одну матрицу обратить, когда производную считали.\n",
    "\n",
    "Сейчас мы проверим, правильно ли считаются производные целевой функции: мы реализуем подсчёт частных производных по определению, как $$\\frac{\\partial f}{\\partial x_i} = \\frac{f\\left(x_1,\\ldots,x_{i-1}, x_i + \\Delta x, x_{i+1}, \\ldots, x_d\\right) - f\\left(x_1,\\ldots, x_d\\right)}{\\Delta x}.$$ Это не определение, в определении был бы $\\lim _{\\Delta x\\to 0}$! Но если мы возьмём достаточно малое $\\Delta x$, то приближение будет неплохим.\n",
    "\n",
    "Иными словами, мы посчитаем целевую функцию, чуть-чуть поменяем какой-нибудь вес, после этого посчитаем целевую функцию еще раз, дальше применяем определение, то есть разделим разницу в целевой функции на изменение веса.\n",
    "\n",
    "После этого можно будет сравнить результаты, полученные с помощью аналитического и численного метода: они не должны сильно отличаться.\n",
    "\n",
    "Вы уже должны догадываться, почему мы не можем всегда обходиться только лишь численным нахождением производной, но пример, непосредственно это иллюстрирующий, появится на следующей неделе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_grad_numerically(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    \"\"\"\n",
    "    Численная производная целевой функции\n",
    "    neuron - объект класса Neuron\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений\n",
    "    y - правильные ответы для тестовой выборки X\n",
    "    J - целевая функция, градиент которой мы хотим получить\n",
    "    eps - размер $\\delta w$ (малого изменения весов)\n",
    "    \"\"\"\n",
    "\n",
    "    initial_cost = J(neuron, X, y)\n",
    "    w_0 = neuron.w\n",
    "    num_grad = np.zeros(w_0.shape)\n",
    "    \n",
    "    for i in range(len(w_0)):\n",
    "        \n",
    "        old_wi = neuron.w[i].copy()\n",
    "        # Меняем вес\n",
    "        neuron.w[i] += eps\n",
    "        \n",
    "        # Считаем новое значение целевой функции и вычисляем приближенное значение градиента\n",
    "        num_grad[i] = (J(neuron, X, y) - initial_cost)/eps\n",
    "        \n",
    "        # Возвращаем вес обратно. Лучше так, чем -= eps, чтобы не накапливать ошибки округления\n",
    "        neuron.w[i] = old_wi\n",
    "            \n",
    "    # проверим, что не испортили нейрону веса своими манипуляциями\n",
    "    assert np.allclose(neuron.w, w_0), \"МЫ ИСПОРТИЛИ НЕЙРОНУ ВЕСА\"\n",
    "    return num_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что у нас получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Подготовим данные\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "X = np.hstack((np.ones((len(y), 1)), X))\n",
    "y = y.reshape((len(y), 1)) # Обратите внимание на эту очень противную и важную строчку\n",
    "\n",
    "\n",
    "# Создадим нейрон\n",
    "\n",
    "w = np.random.random((X.shape[1], 1))\n",
    "neuron = Neuron(w, activation_function=sigmoid, activation_function_derivative=sigmoid_prime)\n",
    "\n",
    "# Посчитаем пример\n",
    "num_grad = compute_grad_numerically(neuron, X, y, J=J_quadratic)\n",
    "an_grad = compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative)\n",
    "\n",
    "print(\"Численный градиент: \\n\", num_grad)\n",
    "print(\"Аналитический градиент: \\n\", an_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Вроде бы похоже\", но это не очень удовлетворительный ответ. Давайте посмотрим, как меняется наше приближение в зависимости от $\\varepsilon$. Посчитаем для разных $\\varepsilon$ модуль разности этих двух градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_grad_diff(eps):\n",
    "    num_grad = compute_grad_numerically(neuron, X, y, J=J_quadratic, eps=float(eps))\n",
    "    an_grad = compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative)\n",
    "    print(np.linalg.norm(num_grad-an_grad))\n",
    "    \n",
    "interact(print_grad_diff, \n",
    "            eps=RadioButtons(options=[\"3\", \"1\", \"0.1\", \"0.001\", \"0.0001\"]), separator=\" \");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо, но можно лучше, причем с минимумом усилий.\n",
    "Давайте вместо того, чтобы считать $\\frac{f(x) - f(x + \\Delta x)}{\\Delta x}$, посмотрим на $\\frac{f(x  + \\Delta x) - f(x - \\Delta x)}{2 \\Delta x}$, то есть шагнём в обе стороны. Говорят, что на практике этот метод работает лучше.\n",
    "\n",
    "Реализуйте функцию compute_grad_numerically_2, чтобы проверить, слухи это или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_grad_numerically_2(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    \"\"\"\n",
    "    Численная производная целевой функции.\n",
    "    neuron - объект класса Neuron с вертикальным вектором весов w,\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений,\n",
    "    y - правильные ответы для тестовой выборки X,\n",
    "    J - целевая функция, градиент которой мы хотим получить,\n",
    "    eps - размер $\\delta w$ (малого изменения весов).\n",
    "    \"\"\"\n",
    "    \n",
    "    # эту функцию необходимо реализовать\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, к какому результату привели ваши эксперименты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_grad_diff_2(eps):\n",
    "    num_grad = compute_grad_numerically_2(neuron, X, y, J=J_quadratic, eps=float(eps))\n",
    "    an_grad = compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative)\n",
    "    print(np.linalg.norm(num_grad-an_grad))\n",
    "    \n",
    "interact(print_grad_diff_2, \n",
    "            eps=RadioButtons(options=[\"3\", \"1\", \"0.1\", \"0.001\", \"0.0001\"]), separator=\" \");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами тот редкий случай, когда фраза **\"на порядок лучше\"** - очевидное преуменьшение!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы сидели, уставшие, но довольные проделанной работой, как вдруг Зюк оживлённо воскликнул:\n",
    "\n",
    "-- У нас ведь всего два входа! \n",
    "\n",
    "-- И что? - удивлённо спросили мы. \n",
    "\n",
    "-- А то, что если мы зафиксируем значение смещения, то можем полноценно визуализировать целевую функцию. Неужели вам не хочется посмотреть, как она выглядит в этой задачке? А еще.. А еще давайте добавим возможность перемешивать и сдвигать данные! Как вам такая идея?\n",
    "\n",
    "Не дождавшись ответа, Зюк начал программировать. Он выглядел очень возбуждённым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J_by_weights(weights, X, y, bias):\n",
    "    \"\"\"\n",
    "    Посчитать значение целевой функции для нейрона с заданными весами.\n",
    "    Только для визуализации\n",
    "    \"\"\"\n",
    "    new_w = np.hstack((bias, weights)).reshape((3,1))\n",
    "    return J_quadratic(Neuron(new_w), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "max_b = 40\n",
    "min_b = -40\n",
    "max_w1 = 40\n",
    "min_w1 = -40\n",
    "max_w2 = 40\n",
    "min_w2 = -40\n",
    "\n",
    "g_bias = 0 # график номер 2 будет при первой генерации по умолчанию иметь то значение b, которое выставлено в первом\n",
    "X_corrupted = X.copy()\n",
    "y_corrupted = y.copy()\n",
    "\n",
    "@interact(fixed_bias=FloatSlider(min=min_b, max=max_b, continuous_update=False), \n",
    "          mixing=FloatSlider(min=0, max=1, continuous_update=False, value=0),\n",
    "          shifting=FloatSlider(min=0, max=1, continuous_update=False, value=0)\n",
    "            )\n",
    "def visualize_cost_function(fixed_bias, mixing, shifting):\n",
    "    \"\"\"\n",
    "    Визуализируем поверхность целевой функции на (опционально) подпорченных данных и сами данные.\n",
    "    Портим данные мы следующим образом: сдвигаем категории навстречу друг другу, на величину, равную shifting \n",
    "    Кроме того, меняем классы некоторых случайно выбранных примеров на противоположнее.\n",
    "    Доля таких примеров задаётся переменной mixing\n",
    "    \n",
    "    Нам нужно зафиксировать bias на определённом значении, чтобы мы могли что-нибудь визуализировать.\n",
    "    Можно посмотреть, как bias влияет на форму целевой функции\n",
    "    \"\"\"\n",
    "    xlim = (min_w1, max_w1)\n",
    "    ylim = (min_w2, max_w2)\n",
    "    xx = np.linspace(*xlim, num=101)\n",
    "    yy = np.linspace(*ylim, num=101)\n",
    "    xx, yy = np.meshgrid(xx, yy)\n",
    "    points = np.stack([xx, yy], axis=2)\n",
    "    \n",
    "    # не будем портить исходные данные, будем портить их копию\n",
    "    corrupted = data.copy()\n",
    "    \n",
    "    # инвертируем ответы для случайно выбранного поднабора данных\n",
    "    mixed_subset = np.random.choice(range(len(corrupted)), int(mixing * len(corrupted)), replace=False)\n",
    "    corrupted[mixed_subset, -1] = np.logical_not(corrupted[mixed_subset, -1])\n",
    "    \n",
    "    # сдвинем все груши (внизу справа) на shifting наверх и влево\n",
    "    pears = corrupted[:, 2] == 1\n",
    "    apples = np.logical_not(pears)\n",
    "    corrupted[pears, 0] -= shifting\n",
    "    corrupted[pears, 1] += shifting\n",
    "    \n",
    "    # вытащим наружу испорченные данные\n",
    "    global X_corrupted, y_corrupted\n",
    "    X_corrupted = np.hstack((np.ones((len(corrupted),1)), corrupted[:, :-1]))\n",
    "    y_corrupted = corrupted[:, -1].reshape((len(corrupted), 1))\n",
    "    \n",
    "    # посчитаем значения целевой функции на наших новых данных\n",
    "    calculate_weights = partial(J_by_weights, X=X_corrupted, y=y_corrupted, bias=fixed_bias)\n",
    "    J_values = np.apply_along_axis(calculate_weights, -1, points)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,5))\n",
    "    # сначала 3D-график целевой функции\n",
    "    ax_1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    surf = ax_1.plot_surface(xx, yy, J_values, alpha=0.3)\n",
    "    ax_1.set_xlabel(\"$w_1$\")\n",
    "    ax_1.set_ylabel(\"$w_2$\")\n",
    "    ax_1.set_zlabel(\"$J(w_1, w_2)$\")\n",
    "    ax_1.set_title(\"$J(w_1, w_2)$ for fixed bias = ${}$\".format(fixed_bias))\n",
    "    # потом плоский поточечный график повреждённых данных\n",
    "    ax_2 = fig.add_subplot(1, 2, 2)\n",
    "    plt.scatter(corrupted[apples][:, 0], corrupted[apples][:, 1], color = \"red\", alpha=0.7)\n",
    "    plt.scatter(corrupted[pears][:, 0], corrupted[pears][:, 1], color = \"green\", alpha=0.7)\n",
    "    ax_2.set_xlabel(\"yellowness\")\n",
    "    ax_2.set_ylabel(\"symmetry\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы с интересом рассматривали результат. По форме функции становилось понятно, что градиентный спуск будет обычно попадать в ловушку. На графике видна огромная яма, прижимающаяся своим дном к нулю. \n",
    "\n",
    "Это происходит от того, что данные линейно разделимы, и мы всегда можем увеличить хорошие веса, получив еще более хорошие (подумайте, почему это так).\n",
    "\n",
    "Еще страшнее выглядят плоские участки. Как вы думаете, застрянем мы не них или нет?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим графиком Зюк особенно нас порадовал. Можно было выбрать точку и bias, откуда мы хотим начать градиентный спуск, после чего его программа генерировала \"историю обучения\" (learning curve) - график значений целевой функции после очередного обновления весов. \n",
    "\n",
    "Смотря на поверхность целевой функции мы старались предсказать, к каким весам мы придём, если начнём спуск с той или иной точки. Оказалось что всё не так очевидно, как мы думали.\n",
    "\n",
    "На графике ниже видно, что при \"хорошем\" раскладе (мы попали в большую яму) значение целевой функции всё время уменьшалось (хоть и по чуть-чуть), пока мы не достигали порога по количеству итераций. При плохом мы могли просто застрять в локальном минимуме или выйти на плато. \n",
    "\n",
    "\\\\\\\\ Как можно бороться с подобными ловушками мы поговорим на третьей и четвёртой неделях.\n",
    "\n",
    "***Если в предыдущем задании вы \"подпортили\" данные - график будет строиться именно для подпорченных =)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact(b=BoundedFloatText(value=str(g_bias), min=min_b, max=max_b, description=\"Enter $b$:\"),\n",
    "          w1=BoundedFloatText(value=\"0\", min=min_w1, max=max_w1, description=\"Enter $w_1$:\"),\n",
    "          w2=BoundedFloatText(value=\"0\", min=min_w2, max=max_w2, description=\"Enter $w_2$:\"),\n",
    "          learning_rate=Dropdown(options=[\"0.01\", \"0.05\", \"0.1\", \"0.5\", \"1\", \"5\", \"10\"], \n",
    "                                value=\"0.01\", description=\"Learning rate: \")\n",
    "         )\n",
    "def learning_curve_for_starting_point(b, w1, w2, learning_rate=0.1):\n",
    "    w = np.array([b, w1, w2]).reshape(X_corrupted.shape[1], 1)\n",
    "    learning_rate=float(learning_rate)\n",
    "    neuron = Neuron(w, activation_function=sigmoid, activation_function_derivative=sigmoid_prime)\n",
    "\n",
    "    story = [J_quadratic(neuron, X_corrupted, y_corrupted)]\n",
    "    for _ in range(2000):\n",
    "        neuron.SGD(X_corrupted, y_corrupted, 2, learning_rate=learning_rate, max_steps=2)\n",
    "        story.append(J_quadratic(neuron, X_corrupted, y_corrupted))\n",
    "    plt.plot(story)\n",
    "    \n",
    "    plt.title(\"Learning curve.\\n Final $b={0:.3f}$, $w_1={1:.3f}, w_2={2:.3f}$\".format(*neuron.w.ravel()))\n",
    "    plt.ylabel(\"$J(w_1, w_2)$\")\n",
    "    plt.xlabel(\"Weight and bias update number\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Близился вечер, и мы решили, что на сегодня уже утолили жажду программирования и познания. \n",
    "Даже Зюк выглядел немного утомлённым, насколько можно было судить по его инопланетной мимике.\n",
    "Он пообещал, что в следующий раз мы сможем сделать настоящую нейросеть, и мы отправились спать,\n",
    "думая о предстоящей встрече.\n",
    "\n",
    "Однако, не все. Кто-то из наших так увлёкся, что уже сейчас, не дожидаясь следующей встречи, отправился заменять целевые и активационные функции нейронов, наблюдая, как это отразится на результатах.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
